# 軽量Transformer言語モデルにおける感情回路の因果解析

— 残差方向・サブスペース構造・注意ヘッド介入・中規模モデルスケーリングによる多段階研究 —

（2025年11月16日版）

## 今回の更新点（2025-11-16）

- **Phase 7.5 統計的検証の完了**: 効果量（Cohen's d）、p値、検出力分析、k選択のブートストラップ検証を実施
- **Phase 8 中規模モデルスケーリングの完了**: Llama3 8B、Gemma3 12B、Qwen3 8B でGPT-2とのサブスペースアライメントを実験
- **Phase 8 比較解析の完了**: 3モデル間の整合性比較とモデル間一般性の検証
- **統計モジュールの実装**: `run_statistics.py`による一括統計処理パイプラインを構築
- **中規模モデルパイプラインの実装**: `run_phase8_pipeline.py`と`summarize_phase8_large.py`による自動化を実現

## 要旨

本研究では，軽量の Transformer 言語モデルが感情情報をどのように内部表現し，その表現が生成テキストにどの程度因果的な影響を与えるかを検証した。対象は GPT‑2（124M），Pythia‑160M，GPT‑Neo‑125M の３モデルに加え，中規模モデルとして Llama3 8B，Gemma3 12B，Qwen3 8B を含む。手作業で作成した感謝・怒り・謝罪・中立の短文からなる均衡データセットに対して各層の残差ストリーム活性を抽出し，感情ベクトルを算出した。さらに，モデル間サブスペースの比較，残差ストリームへのベクトル追加によるパッチング実験，注意ヘッド単位の介入，統計的検証，中規模モデルへのスケーリングを行った。

その結果，(1) 感情語トークンの残差方向が安定した感情ベクトルを形成すること，(2) モデル間の感情サブスペース重なりが無作為ベースラインより有意に高い（0.13〜0.15）こと，(3) 中立空間で学習した線形写像により cos² が ≈0.001 から ≈0.99 へと大幅に向上すること，(4) 層×α スイープによるパッチングが生成文の感情的なトーンを変えること，(5) Layer 1 Head 10 など特定の注意ヘッドが感情処理に強く反応し，ヘッドレベルの介入で感情的な出力を増強できること，(6) 効果量は small effect 未満（Cohen's d < 0.2）だが方向性は一貫していること，(7) k=2 で overlap が最大になることが統計的に確認されたこと，(8) Llama3 8B では GPT-2 との整合性が非常に高く（Δ=0.71），中規模モデルでも同様のパターンが確認されたことを示した。以上より，軽量モデルにも分散的だが部分的に分離可能な「感情回路」が存在し，その構造は中規模モデルにもある程度一般化可能であることを示した。

## 1. 序論

Transformer 言語モデルの内部機構を理解する機械論的解釈は，モデルの信頼性向上に不可欠である。近年，大規模モデルにおいて感情が低次元サブスペースとして符号化されることが示唆されているが，軽量モデルについては，感情方向の存在やその因果的影響，感情処理に関わる注意ヘッドの特定といった点が十分に検証されていない。本研究では，Phase1〜8 の実験を通じて，次の研究課題を解明した。

* RQ1: 軽量Transformerに感情方向が存在するか。
* RQ2: 感情サブスペースはモデル間で共通するか。
* RQ3: 感情方向への介入は生成文のスタイルを変えるか。
* RQ4: 特定の注意ヘッドは感情処理に因果的な役割を持つか。
* RQ5: 小型モデルで見つかったパターンは中規模モデルにも一般化するか。

以下では，各フェーズで得られた知見を基に現時点の理解をまとめ，今後の課題を論じる。

## 2. データセット構築（Phase 1）

感情データセットは感謝・怒り・謝罪・中立の４カテゴリからなる。Baseline プロファイルでは各感情70文，Extended プロファイルでは各100文を作成し，計680文を用意した。平均トークン長は約7.1トークンであり，すべて英語文である。バランスが完全に取れており，感情語が明示されているため，後続の機械論的実験に適する。

## 3. 活性抽出（Phase 2）

各モデルについて，各層の残差ストリームと MLP 出力を全サンプルに対して保存した。GPT‑2，Pythia‑160M，GPT‑Neo‑125M はいずれも12層・隠れ次元768である。抽出された活性テンソルは `[n_layers][n_samples][seq_len, d_model]` の形状を持ち，合計約431 MBとなった。注意重みはメモリ節約のため保存していない。

## 4. 感情ベクトルの構築（Phase 3）

### 4.1 手法

感情ベクトルは２種類の方法で算出した。１つは文章末の残差を平均する sentence‑end 手法，もう１つは感情語トークン位置の残差を用いる token‑based 手法である。token‑based 手法は感情語に直接対応するため，安定性と解釈性が高いと評価された。

### 4.2 類似度とベクトル強度

Gratitude と Anger の cosine 類似度は，sentence‑end 手法で 0.3706，token‑based 手法では 0.1638 と大きく低下し，token‑based 手法の方が異なる感情をより明確に区別できることが示された。層が深くなるほど token‑based ベクトルの L1/L2 ノルムは増加し，特に Layer 9〜11 で強い感情表現が現れた。

## 5. サブスペース解析とアライメント（Phase 3.5）

### 5.1 サブスペースの重なり

各モデルごとに k=10 の PCA 感情サブスペースを求め，モデル間のサブスペース重なりを評価した。GPT‑2 と Pythia‑160M の平均 overlap は 0.1496，GPT‑2 と GPT‑Neo‑125M は 0.1449，Pythia‑160M と GPT‑Neo‑125M は 0.1397 であり，ランダムベースライン（0.0〜0.1）より高い値を示した。これはモデル間で共通する感情構造が存在することを意味する。

### 5.2 Neutral Alignment と Procrustes

中立プロンプトの残差から線形写像を学習し，GPT‑2 と Pythia‑160M の表現空間を整列したところ，before alignment では cos² ≈0.001 だったものが after alignment では ≈0.99 まで向上した。Procrustes alignment では改善率約7%と限定的だった。これにより，「座標系は異なるが本質的な構造は同一である」という仮説が支持された。

### 5.3 k‑sweep

サブスペース次元 k を 2・5・10・20 と変化させた実験では，k=2 で overlap が最も高く 0.0028 となり，低次元に感情のコア因子が存在する可能性が示された。

## 6. 感情パッチング（Phase 4–5）

### 6.1 残差方向へのパッチング

neutral プロンプトに対し，残差ストリームに感情ベクトルを加算するパッチングを実施した。層6かつ α=1.0 では生成文が感情的なスタイルに変化するものの，繰り返し語句が多くなり過剰な影響が観察された。multi‑token 生成（10〜30トークン）を行うと，単一トークンでは検出できない感情的なスタイル変化が顕在化した。

### 6.2 層×αスイープ

GPT‑2 を対象に層 (3, 5, 7, 9, 11) と α 値 (−2, −1, −0.5, 0, 0.5, 1, 2) を網羅的に変化させ，生成文を sentiment / politeness / emotions モデルで評価した。深い層（9, 11）で特に強い効果が現れ，α は 0.5〜1.0 の範囲が適切と考えられた。α=2.0 では生成が崩壊するケースが多かった。

## 7. ヘッドスクリーニング（Phase 6）

全注意ヘッドについて，感情語トークンへの Δattention を測定し重要度をランキングした。gratitude 感情では Layer 1 Head 10 が最も高い Δattention 0.340434 を示し，Layer 3 Head 2 や Layer 1 Head 11 がこれに続いた。浅い層（0–1）と深い層（11）で感情語に反応するヘッドが多いことが分かった。

ヘッド ablation 実験では Layer 1 Head 10 を無効化したところ，gratitude キーワード出現回数が78 → 76 にわずかに減少したものの，sentiment スコアには大きな変化がなかった。これは単一ヘッドの ablation では影響が限定的であることを示す。

## 8. ヘッドパッチング（Phase 7）

Layer 1 Head 10 への causal patching を行い，感情的スタイルへの影響を測定した。pattern_v モードでパッチングすると，Sentiment の平均が 0.5832 → 0.5981 と +0.0149 上昇し，gratitude キーワード総数が 4 → 8 と倍増した。v_only モードでも同程度の効果が得られた。これは Head Screening による相関的エビデンスが因果的に検証されたことを意味する。

なお，複数ヘッドの同時パッチングは今後の課題として残されている。

## 9. 統計的検証（Phase 7.5）

### 9.1 効果量の評価

Headパッチング実験（Layer 0 Head 0、gratitude感情）における主要な結果：

| メトリクス | mean_diff | Cohen's d | 95% CI | p値 | 有意性 (FDR) |
|-----------|-----------|-----------|--------|-----|--------------|
| emotion_keywords.gratitude | +0.057 | 0.102 | [-0.071, 0.193] | 0.398 | No |
| sentiment | +0.026 | 0.079 | [-0.052, 0.104] | 0.509 | No |
| politeness | +0.001 | 0.027 | [-0.009, 0.014] | 0.820 | No |
| emotion_keywords.anger | -0.014 | -0.053 | [-0.071, 0.057] | 0.658 | No |

**解釈**: 効果量はすべて small effect 未満（d < 0.2）だが，gratitude キーワードの増加（+0.057）と sentiment の上昇（+0.026）において方向性は一貫している。統計的に有意ではないが，これは検出力の不足（平均9.1%）が原因である可能性が高い。

### 9.2 検出力分析

現在のサンプルサイズ（n=70）における事後的検出力：

- 平均検出力: 9.1%（非常に低い）
- Small effect (d=0.2) を検出するには: 225サンプル以上が必要
- Medium effect (d=0.5) を検出するには: 36サンプルで十分（現在のサンプル数で対応可能）

### 9.3 k選択の統計的確認

感情サブスペース次元 k を変化させた実験で，k=2 で overlap が最大になることをブートストラップ（500回）で統計的に確認：

| k | anger overlap | apology overlap | gratitude overlap |
|---|--------------|-----------------|-------------------|
| 2 | 0.00239 | **0.00333** | 0.00274 |
| 5 | 0.00132 | 0.00183 | 0.00152 |
| 10 | 0.00124 | 0.00140 | 0.00130 |
| 20 | 0.00129 | 0.00134 | 0.00125 |

**重要な発見**: k=2 から k=5 への減少率は約40-45%と大きく，感情表現のコア因子が2次元に凝縮されていることを強く示唆。Baseline と Extended プロファイルで同様の傾向が確認され，データセットサイズに依存しない頑健な発見である。

## 10. 中規模モデルスケーリング（Phase 8）

### 10.1 実験設定

- **Sourceモデル**: GPT-2 small (124M)
- **Targetモデル**: Llama3 8B, Gemma3 12B, Qwen3 8B
- **手法**: token-based感情ベクトル → 多サンプルPCA → neutralサブスペースからの線形写像 → before/after overlap

### 10.2 Llama3 8B の結果

| Layer | Before | After | Δ |
|-------|--------|-------|---|
| 0 | 0.000215 | 0.124622 | 0.124407 |
| 1 | 0.000180 | 0.504833 | **0.504653** |
| 9 | 0.000193 | 0.573908 | **0.573715** |
| 11 | 0.000210 | 0.713503 | **0.713292** |

**平均改善幅**: 0.461（before: 0.000215 → after: 0.460810）

**解釈**: すべての層で大幅な改善が見られ，特に上位層（Layer 9-11）で改善が大きい。小型モデル（GPT-2 vs Pythia-160M）で観察された「before ≒ 0 → after > 0.1」パターンと完全に一致。

### 10.3 Qwen3 8B の結果

| Layer | Before | After | Δ |
|-------|--------|-------|---|
| 1 | 0.000217 | 0.077358 | 0.077141 |
| 3 | 0.000216 | 0.085964 | 0.085748 |
| 4 | 0.000223 | 0.105332 | **0.105109** |
| 11 | 0.000229 | 0.081099 | 0.080870 |

**平均改善幅**: 0.055（before: 0.000226 → after: 0.054855）

**解釈**: 改善は確認されるが，Llama3ほど大きくない。中間層（Layer 1-4）で最大の改善が見られ，小型モデルと部分的に類似したパターン。

### 10.4 Gemma3 12B の結果

| Layer | Before | After | Δ |
|-------|--------|-------|---|
| 0 | 0.000509 | 0.000011 | **-0.000499** |
| 5 | 0.000259 | 0.000554 | 0.000294 |

**平均改善幅**: 0.000052（before: 0.000299 → after: 0.000351）

**解釈**: 改善がほぼ見られない。数値的な問題（活性値のスケールが大きい）が発生し，Layer 7-11は有効サンプル不足のためスキップ。

### 10.5 モデル間比較の示唆

| モデル | 最大Δ | 最大Δの層 | 平均Δ | パターン一致 |
|--------|-------|----------|-------|-------------|
| Llama3 8B | **0.713** | Layer 11 | **0.461** | ✅ 完全一致 |
| Qwen3 8B | 0.105 | Layer 4 | 0.055 | ⚠️ 部分一致 |
| Gemma3 12B | 0.0003 | Layer 5 | 0.000052 | ❌ 不一致 |

**主要な発見**:
1. Llama3 8B では小型モデルのパターンが完全に再現され，中規模モデルでも「座標系が違うだけで本質は同じ」という構造が確認された
2. Qwen3 8B では部分的に再現されるが，改善幅は小さい
3. Gemma3 12B では現在の手法では効果が限定的であり，アーキテクチャ差が大きく影響

## 11. 議論

本研究を通じて，軽量 Transformer における感情処理は次のような流れで行われることが示唆された。まず，浅い層（Layer 0〜1）の特定ヘッドが感情語トークンに注意を集中し，残差ストリームに感情方向を刻み込む。中間層から深層にかけて感情方向が蓄積され，Layer 9〜11 で生成スタイルとして顕在化する。特定のヘッド（Layer 1 Head 10 等）への介入により，生成文の感情トーンを因果的に操作できることが確認された。

さらに，中規模モデル（特に Llama3 8B）でも同様のパターンが確認され，感情サブスペースの構造はモデルサイズに依存しない普遍的な特性である可能性が示された。ただし，モデルファミリー（Llama / Gemma / Qwen）によってアライメントの成功度が異なり，アーキテクチャ差が重要な要因であることも明らかになった。

## 12. 限界

本研究は 125M〜160M パラメータの軽量モデルと 8B〜12B パラメータの中規模モデルを対象としており，より大規模なモデル（70B以上）に同様の回路が存在するかは未検証である。また，残差パッチングでは繰り返し語句など自然性の低下が見られた。効果量が small effect 未満であり，検出力が不足しているため，統計的有意性の確保にはより大きなサンプルサイズが必要である。Gemma3 12B では数値的な問題が発生し，現在の手法の限界が示された。ヘッドの OV/QK 分解や複数ヘッドの組み合わせ効果，多言語への一般化，より高度なアライメント手法（CCA、非線形写像）は今後の課題である。

## 13. 今後の研究

今後は以下の点に取り組む予定である。

1. **サンプルサイズの拡大**: Small effect を検出するために225サンプル以上を収集
2. **OV/QK 回路解析**: 重要なヘッドがどのようなベクトルを書き込み，どのトークン間で注意を張るかを分析
3. **複数ヘッドの再構成**: 分散的な回路を抽出し，ヘッドの協調作用を明らかにする
4. **Gemma3 12B の再検討**: より高度なアライメント手法（CCA、非線形写像）の検討
5. **より大規模なモデルへの展開**: 13B以上のモデルでの検証
6. **ランダムベクトル対照の強化**: 感情方向とランダム方向を比較し，因果効果の特異性を検証
7. **生成の自然性評価**: 人間評価を用いて，パッチング後の文の自然さと感情表現を詳細に評価

## 14. 結論

本研究は，軽量 Transformer 言語モデルにおいて感情方向が明確に存在し，モデル間で共通するサブスペース構造があること，残差ストリームへのパッチングや注意ヘッドへの介入により生成文の感情トーンを因果的に操作できることを実証した。特に，Layer 1 Head 10 のようなヘッドが感情処理に大きく寄与していることが示された。

Phase 7.5 の統計的検証により，効果量は small effect 未満だが方向性は一貫していること，k=2 で overlap が最大になることが統計的に確認された。Phase 8 の中規模モデルスケーリングにより，Llama3 8B では小型モデルのパターンが完全に再現され，感情サブスペースの構造がモデルサイズに依存しない普遍的な特性である可能性が示された。

これらの知見は，感情の内部回路理解に向けた基盤となるものであり，今後の機械論的解釈やモデル制御の研究に資することが期待される。
