# 小型モデルにおける感情表現方向の抽出

近年、GPT-2 や Pythia 等の小型Transformerでは、入力文や単語の感情極性を一つの線形方向（感情方向）として表現できることが示されている。例えば、Geiger ら（2024）はGPT-2 small（124M）の各トークン残差ストリームに対し、少数のポジティブ/ネガティブ形容詞（30語）を用いてK-means クラスタリングし、感情方向を見出した[arxiv.org](https://arxiv.org/pdf/2310.15154#:~:text=3,as%20expected%20since%20the%20residual)。この感情方向に対する投影値はトークンの評価（正負）をかなり高精度に分類し、K-means法で約78%の識別精度を示す。他の手法（主成分分析、平均差分、ロジスティック回帰、DAS）も類似の方向を得ており、それぞれ80～89%の精度を達成している[arxiv.org](https://arxiv.org/pdf/2310.15154#:~:text=Activation%20Cum,means)。さらに、これら異なる手法で得られた方向ベクトル間には極めて高いコサイン類似度があり、実質的に同一の「感情方向」を捉えていることが確認されている[arxiv.org](https://arxiv.org/pdf/2310.15154#:~:text=3,as%20expected%20since%20the%20residual)。すなわち、小型モデルでも感情概念は入力空間に線形に符号化されやすいことが示唆されている。これらの検証は主に残差ストリームに対して行われたが、同様の方向の存在は埋め込み空間にも仮定できる（今後の検討課題）。

 また、感情方向が単語の意味ベクトルと独立した情報を保持している可能性も示唆されている。Geiger ら（2024）は、文中の「要約点」（句読点や特定名詞）に感情情報が集約される“summarization motif”も報告しており、コンマ位置での感情方向活性化を抑制する実験では文書分類タスクの正解率が大幅に低下した[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=names,reducing%20accuracy%20to%2082)。これらの結果は、感情方向を用いた表現学習が小型モデルでも有効であることを示している。なお、感情語彙以外にも、関係する感情概念（喜怒哀楽など）の方向を探す手法としては、標語（polar dictionaries）や事前学習済み分類器の内部表現を用いる方法等があり得るが、小型LLMに特化した研究例は今のところ少ない。

# サブスペースのモデル間類似性とアライメント

複数モデル間で感情表現が共有されるかについては、部分的に示唆されている事例がある。例えば、Geiger らの報告では、GPT-2やPythia、さらにはGemmaやQwenといった別系統のモデルでも同様の感情回路が再現可能であるとし、「これらの発見はGPT2, Pythia, Gemma, Qwen, StableLMモデル間で再現された」ことを確認している[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=Section%202,2%20Methods)。つまり、アーキテクチャや規模の異なるモデルでも感情方向に対応する低次元サブスペースがある程度共通している可能性を示唆する。ただし、これは同一訓練データ上で独立に学習した同系モデル間の話であり、完全に異なるモデル（例：GPT系 vs BERT系）間で感情サブスペースを直接比較した報告はまだ少ない。

 また、Geiger らは**DAS（Distributed Alignment Search）**という技術を提案し、データセットやモデル間で特徴空間を最適に回転整列させる手法を用いている[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=DAS%20Distributed%20Alignment%20Search%20,the%20objective%20to%20be%20minimised)。感情サブスペースに限らず、Li ら（2016）もニューラルネットが同一低次元サブスペースを学習することを指摘しており、学習したサブスペースを線形写像で結合すると性能劣化が小さいことも示唆されている[openreview.net](https://openreview.net/pdf/24a91c5b77ff9fda9903cf61b0a51f723a251740.pdf#:~:text=similar%20representations,2021)。これらはサブスペースの一般論であるが、小型LLMでもこれを仮定できる。対照的に、大規模言語モデルでは、複数言語・複数ドメインにわたって「普遍的な感情サブスペース」が存在するとの報告もある。Reichman ら（2025）の解析では、多様な感情データセットで隠れ状態を調査し、データ間で整列誤差が小さく汎化性能が高い普遍的な感情サブスペースを見出している[arxiv.org](https://arxiv.org/html/2510.22042v1#:~:text=Cross,consistent%20and%20manipulable%20affective%20geometry)。これらの研究は主に巨大モデルを対象とするが、構造的に似通った結果が小型モデルにもあることが期待される。総じて、小型モデル固有の系統的なアライメント研究は未成熟であり、今後の研究課題と言える。

# 残差パッチングによる因果操作

感情方向が同定できれば、その方向を残差ストリームに **パッチング** （介入）することで生成結果を操作できる。Geiger ら（2024）では、GPT-2 small において元の入力列と極性を反転させた対照入力列を用意し、残差ストリームの特定層における感情方向成分のみを置換する実験を行った。その結果、入力文の形容詞・動詞位置（ADJ, VRB）に沿った感情方向を逆転させるだけで、モデルの予測ラベルが58.3%のケースで反転した（ロジット差は54.8%減少）[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=To%20further%20validate%20this%20circuit,responsible%20for%20the%20majority%20of)。これは「感情方向」がモデルの判断に因果的に寄与していることを示す。さらに、残差全成分を置換する場合には97%が反転し、75%のロジット差低下が得られた[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=To%20further%20validate%20this%20circuit,responsible%20for%20the%20majority%20of)。つまり感情方向を含むサブスペースを操作することで、生成内容の感情傾向を大規模に制御できるわけである。類似の手法は大規模モデルでも報告されており、Reichman らは指標付き回帰を用いて得られた感情ベクトルを中間層に加える手法でLLMの感情表現を操作可能と報告している（国際認知科学会議2025）。小型モデルではまだ研究例は限られるものの、上述のように実験的に因果制御が確認されている。

 また、Geiger らは句読点や名詞など“要約点”における感情情報の重要性にも注目しており、コンマやピリオド位置の残差を介して感情情報を蓄積する経路が存在することを指摘している[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=names,reducing%20accuracy%20to%2082)[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=both%20direct%20use%20of%20sentiment,SUM%20and%20write%20to%20END)。実際、句読点位置での感情方向のみをゼロ化する実験では、ムービーレビューの感情分類精度が100%→82%に低下した[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=names,reducing%20accuracy%20to%2082)。これは、感情を要約する中間トークンを破壊すると出力が変化する例であり、残差パッチングによる介入（方向成分の追加・除去）が感情表現を効果的に操作することを示す。

# 注意ヘッドへの因果介入事例

モデル内部の要素単位では、いくつかの研究で特定の注意ヘッドが感情処理に関与していることが示されている。前述のGPT-2 small のセンチメント回路では、層7のヘッド7.1および7.5が感情情報を「要約トークン（SUM）」に書き込む動作を行い、この情報を層10-11のヘッド（読者）が用いて最終予測に寄与していた[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=both%20direct%20use%20of%20sentiment,SUM%20and%20write%20to%20END)。すなわち、7.1, 7.5の2つのヘッドが感情的語彙から情報を引き出し、終端トークンに集約している。これらヘッド経路を介して生成される情報はロジットに大きな影響を与えており、実際に回路全体に残差パッチングを行うと先述の通り予測が大きく反転する[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=To%20further%20validate%20this%20circuit,responsible%20for%20the%20majority%20of)。直接的なヘッド摘出（アブレーション）は同研究では行われていないものの、ヘッドの出力を凍結・置換する類似の介入で因果効果が確かめられている。

 一方、感情ヘッドに特化した研究はほとんどない。GPT-2 small では例えば層10ヘッド7（L10H7）が**コピー抑制**の機能を担うことが報告されている[arxiv.org](https://arxiv.org/pdf/2310.04625#:~:text=war%E2%80%9D%29%20and%20decrease%20model%20loss,its%20input%20stimulus%20and%20specific)が、これは感情とは別課題である。他のトランスフォーマモデルでも、注意ヘッドの値ベクトルやパターンをパッチングして機能解析する手法が用いられている。例えば上述の文脈ではコンマ位置へのパッチングが感情出力を変化させており[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=names,reducing%20accuracy%20to%2082)、注意ヘッドの役割を介して感情特徴が伝播していると考えられる。総じて、小型モデルでは感情表現は単一ヘッドではなく複数ヘッドの協調動作で成立している例が多く、個別ヘッドへの介入は回路全体の因果機構解析と合わせて検討されてきた。

# まとめ

以上、100～200M規模の小型Transformerにおける感情表現・処理メカニズムについて整理した。既存研究では**（1） **言語モデル内部で感情や評価が線形方向として符号化される可能性が示されており、これを探索する手法としてK-means・主成分分析・ロジスティック回帰・DASなどが試されてきた[arxiv.org](https://arxiv.org/pdf/2310.15154#:~:text=Activation%20Cum,DAS%2086)[arxiv.org](https://arxiv.org/pdf/2310.15154#:~:text=3,as%20expected%20since%20the%20residual)。** （2） **感情関連サブスペースは複数モデル間でも類似性が見られることが示唆されるが、小型モデル固有の明示的なアライメント研究は限られる（大規模モデルでは普遍的サブスペースが報告[arxiv.org](https://arxiv.org/html/2510.22042v1#:~:text=Cross,consistent%20and%20manipulable%20affective%20geometry)）。** （3） **感情方向に基づく残差ストリームへのパッチングは生成結果の感情性を大きく変化させうることが確認されており[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=To%20further%20validate%20this%20circuit,responsible%20for%20the%20majority%20of)[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=names,reducing%20accuracy%20to%2082)、感情表現の因果的制御が可能である。** （4）**特定ヘッドへの介入は複数ヘッドからなる感情回路の分析と組み合わせて検討されており、GPT-2 small のケースではヘッド7.1,7.5が感情情報の要約に寄与するなどが明らかになっている[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=both%20direct%20use%20of%20sentiment,SUM%20and%20write%20to%20END)。これらは大規模モデルでの知見と共通する点も多いが、小型モデルに特化した分析はまだ初期段階である。今後は、GPT-Neo-125MやPythia-160Mなどの他モデルでも同様の手法を適用し、再現性と特徴の差異を明らかにすることで、本研究の独自性・貢献がさらに強調できるだろう。

 **参考文献:** Geiger ら[arxiv.org](https://arxiv.org/pdf/2310.15154#:~:text=Activation%20Cum,DAS%2086)[arxiv.org](https://arxiv.org/pdf/2310.15154#:~:text=3,as%20expected%20since%20the%20residual), Wang ら[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=both%20direct%20use%20of%20sentiment,SUM%20and%20write%20to%20END)[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=To%20further%20validate%20this%20circuit,responsible%20for%20the%20majority%20of)（BlackboxNLP 2024）, Reichman ら[arxiv.org](https://arxiv.org/html/2510.22042v1#:~:text=Cross,consistent%20and%20manipulable%20affective%20geometry)（2025 arXiv）, Elhage ら[openreview.net](https://openreview.net/pdf/24a91c5b77ff9fda9903cf61b0a51f723a251740.pdf#:~:text=similar%20representations,2021)（2024 TMLR）, Emami ら[aclanthology.org](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=names,reducing%20accuracy%20to%2082)（BlackboxNLP 2024）など。

引用

[![](https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32)https://arxiv.org/pdf/2310.15154](https://arxiv.org/pdf/2310.15154#:~:text=3,as%20expected%20since%20the%20residual)[![](https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32)https://arxiv.org/pdf/2310.15154](https://arxiv.org/pdf/2310.15154#:~:text=Activation%20Cum,means)[![](https://www.google.com/s2/favicons?domain=https://aclanthology.org&sz=32)https://aclanthology.org/2024.blackboxnlp-1.5.pdf](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=names,reducing%20accuracy%20to%2082)[![](https://www.google.com/s2/favicons?domain=https://aclanthology.org&sz=32)https://aclanthology.org/2024.blackboxnlp-1.5.pdf](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=Section%202,2%20Methods)[![](https://www.google.com/s2/favicons?domain=https://aclanthology.org&sz=32)https://aclanthology.org/2024.blackboxnlp-1.5.pdf](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=DAS%20Distributed%20Alignment%20Search%20,the%20objective%20to%20be%20minimised)[https://openreview.net/pdf/24a91c5b77ff9fda9903cf61b0a51f723a251740.pdf](https://openreview.net/pdf/24a91c5b77ff9fda9903cf61b0a51f723a251740.pdf#:~:text=similar%20representations,2021)[![](https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32)Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Modelshttps://arxiv.org/html/2510.22042v1](https://arxiv.org/html/2510.22042v1#:~:text=Cross,consistent%20and%20manipulable%20affective%20geometry)[![](https://www.google.com/s2/favicons?domain=https://aclanthology.org&sz=32)https://aclanthology.org/2024.blackboxnlp-1.5.pdf](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=To%20further%20validate%20this%20circuit,responsible%20for%20the%20majority%20of)[![](https://www.google.com/s2/favicons?domain=https://aclanthology.org&sz=32)https://aclanthology.org/2024.blackboxnlp-1.5.pdf](https://aclanthology.org/2024.blackboxnlp-1.5.pdf#:~:text=both%20direct%20use%20of%20sentiment,SUM%20and%20write%20to%20END)[![](https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32)https://arxiv.org/pdf/2310.04625](https://arxiv.org/pdf/2310.04625#:~:text=war%E2%80%9D%29%20and%20decrease%20model%20loss,its%20input%20stimulus%20and%20specific)[![](https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32)https://arxiv.org/pdf/2310.15154](https://arxiv.org/pdf/2310.15154#:~:text=Activation%20Cum,DAS%2086)
