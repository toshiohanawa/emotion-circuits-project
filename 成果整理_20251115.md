# 軽量Transformer言語モデルにおける感情回路の因果解析

— 残差方向・サブスペース構造・注意ヘッド介入による多段階研究 —

（2025年11月15日版）

## 要旨

本研究では，軽量の Transformer 言語モデルが感情情報をどのように内部表現し，その表現が生成テキストにどの程度因果的な影響を与えるかを検証した。対象は GPT‑2（124M），Pythia‑160M，GPT‑Neo‑125M の３モデルである。手作業で作成した感謝・怒り・謝罪・中立の短文からなる均衡データセットに対して各層の残差ストリーム活性を抽出し，感情ベクトルを算出した。さらに，モデル間サブスペースの比較，残差ストリームへのベクトル追加によるパッチング実験，注意ヘッド単位の介入を行った。その結果，(1) 感情語トークンの残差方向が安定した感情ベクトルを形成すること，(2) モデル間の感情サブスペース重なりが無作為ベースラインより有意に高い（0.13〜0.15）こと，(3) 中立空間で学習した線形写像により cos² が ≈0.001 から ≈0.99 へと大幅に向上すること，(4) 層×α スイープによるパッチングが生成文の感情的なトーンを変えること，(5) Layer 1 Head 10 など特定の注意ヘッドが感情処理に強く反応し，ヘッドレベルの介入で感情的な出力を増強できることを確認した。以上より，軽量モデルにも分散的だが部分的に分離可能な「感情回路」が存在することを示した。

## 1. 序論

Transformer 言語モデルの内部機構を理解する機械論的解釈は，モデルの信頼性向上に不可欠である。近年，大規模モデルにおいて感情が低次元サブスペースとして符号化されることが示唆されているが，軽量モデルについては，感情方向の存在やその因果的影響，感情処理に関わる注意ヘッドの特定といった点が十分に検証されていない。本研究では，Phase1〜7 の実験を通じて，次の研究課題を解明した。

* RQ1: 軽量Transformerに感情方向が存在するか。
* RQ2: 感情サブスペースはモデル間で共通するか。
* RQ3: 感情方向への介入は生成文のスタイルを変えるか。
* RQ4: 特定の注意ヘッドは感情処理に因果的な役割を持つか。

以下では，各フェーズで得られた知見を基に現時点の理解をまとめ，今後の課題を論じる。

## 2. データセット構築（Phase 1）

感情データセットは感謝・怒り・謝罪・中立の４カテゴリからなる。Baseline プロファイルでは各感情70文，Extended プロファイルでは各100文を作成し，計680文を用意した。平均トークン長は約7.1トークンであり，すべて英語文である[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase1_data_report.md#L47-L84)。バランスが完全に取れており，感情語が明示されているため，後続の機械論的実験に適する。

## 3. 活性抽出（Phase 2）

各モデルについて，各層の残差ストリームと MLP 出力を全サンプルに対して保存した[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase2_activations_report.md#L61-L77)。GPT‑2，Pythia‑160M，GPT‑Neo‑125M はいずれも12層・隠れ次元768である[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase2_activations_report.md#L39-L59)。抽出された活性テンソルは `[n_layers][n_samples][seq_len, d_model]` の形状を持ち，合計約431 MBとなった[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase2_activations_report.md#L97-L113)。注意重みはメモリ節約のため保存していない。

## 4. 感情ベクトルの構築（Phase 3）

### 4.1 手法

感情ベクトルは２種類の方法で算出した。１つは文章末の残差を平均する sentence‑end 手法，もう１つは感情語トークン位置の残差を用いる token‑based 手法である[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase3_vectors_report.md#L25-L37)。token‑based 手法は感情語に直接対応するため，安定性と解釈性が高いと評価された[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase3_vectors_report.md#L34-L44)。

### 4.2 類似度とベクトル強度

Gratitude と Anger の cosine 類似度は，sentence‑end 手法で 0.3706，token‑based 手法では 0.1638 と大きく低下し[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase3_vectors_report.md#L45-L63)，token‑based 手法の方が異なる感情をより明確に区別できることが示された。層が深くなるほど token‑based ベクトルの L1/L2 ノルムは増加し[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase3_vectors_report.md#L66-L87)，特に Layer 9〜11 で強い感情表現が現れた。

## 5. サブスペース解析とアライメント（Phase 3.5）

### 5.1 サブスペースの重なり

各モデルごとに k=10 の PCA 感情サブスペースを求め，モデル間のサブスペース重なりを評価した。GPT‑2 と Pythia‑160M の平均 overlap は 0.1496，GPT‑2 と GPT‑Neo‑125M は 0.1449，Pythia‑160M と GPT‑Neo‑125M は 0.1397 であり[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase3.5_subspace_report.md#L34-L39)，ランダムベースライン（0.0〜0.1）より高い値を示した[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase3.5_subspace_report.md#L41-L45)。これはモデル間で共通する感情構造が存在することを意味する。

### 5.2 Neutral Alignment と Procrustes

中立プロンプトの残差から線形写像を学習し，GPT‑2 と Pythia‑160M の表現空間を整列したところ，before alignment では cos² ≈0.001 だったものが after alignment では ≈0.99 まで向上した[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase3.5_subspace_report.md#L65-L75)。Procrustes alignment では改善率約7%と限定的だった[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase3.5_subspace_report.md#L76-L84)。これにより，「座標系は異なるが本質的な構造は同一である」という仮説が支持された。

### 5.3 k‑sweep

サブスペース次元 k を 2・5・10・20 と変化させた実験では，k=2 で overlap が最も高く 0.0028 となり[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase3.5_subspace_report.md#L49-L61)，低次元に感情のコア因子が存在する可能性が示された。

## 6. 感情パッチング（Phase 4–5）

### 6.1 残差方向へのパッチング

neutral プロンプトに対し，残差ストリームに感情ベクトルを加算するパッチングを実施した。層6かつ α=1.0 では生成文が感情的なスタイルに変化するものの，繰り返し語句が多くなり過剰な影響が観察された[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase4_patching_report.md#L59-L64)。multi‑token 生成（10〜30トークン）を行うと，単一トークンでは検出できない感情的なスタイル変化が顕在化した[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase4_patching_report.md#L74-L82)。

### 6.2 層×αスイープ

GPT‑2 を対象に層 (3, 5, 7, 9, 11) と α 値 (−2, −1, −0.5, 0, 0.5, 1, 2) を網羅的に変化させ，生成文を sentiment / politeness / emotions モデルで評価した[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase5_sweep_report.md#L37-L49)。深い層（9, 11）で特に強い効果が現れ，α は 0.5〜1.0 の範囲が適切と考えられた[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase5_sweep_report.md#L93-L99)。α=2.0 では生成が崩壊するケースが多かった。

## 7. ヘッドスクリーニング（Phase 6）

全注意ヘッドについて，感情語トークンへの Δattention を測定し重要度をランキングした。gratitude 感情では Layer 1 Head 10 が最も高い Δattention 0.340434 を示し[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase6_head_screening_report.md#L46-L62)，Layer 3 Head 2 や Layer 1 Head 11 がこれに続いた[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase6_head_screening_report.md#L48-L59)。浅い層（0–1）と深い層（11）で感情語に反応するヘッドが多いことが分かった[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase6_head_screening_report.md#L94-L100)。

ヘッド ablation 実験では Layer 1 Head 10 を無効化したところ，gratitude キーワード出現回数が78 → 76 にわずかに減少したものの，sentiment スコアには大きな変化がなかった[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase6_head_screening_report.md#L76-L84)。これは単一ヘッドの ablation では影響が限定的であることを示す。

## 8. ヘッドパッチング（Phase 7）

Layer 1 Head 10 への causal patching を行い，感情的スタイルへの影響を測定した。pattern_v モードでパッチングすると，Sentiment の平均が 0.5832 → 0.5981 と +0.0149 上昇し，gratitude キーワード総数が 4 → 8 と倍増した[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase7_head_patching_report.md#L48-L56)。v_only モードでも同程度の効果が得られた[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase7_head_patching_report.md#L70-L76)。これは Head Screening による相関的エビデンスが因果的に検証されたことを意味する。

なお，複数ヘッドの同時パッチングは今後の課題として残されている[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase7_head_patching_report.md#L120-L123)。

## 9. 議論

本研究を通じて，軽量 Transformer における感情処理は次のような流れで行われることが示唆された。まず，浅い層（Layer 0〜1）の特定ヘッドが感情語トークンに注意を集中し，残差ストリームに感情方向を刻み込む。中間層から深層にかけて感情方向が蓄積され，Layer 9〜11 で生成スタイルとして顕在化する。特定のヘッド（Layer 1 Head 10 等）への介入により，生成文の感情トーンを因果的に操作できることが確認された。

## 10. 限界

本研究は 125M〜160M パラメータの軽量モデルに限定しており，より大規模なモデルに同様の回路が存在するかは未検証である。また，残差パッチングでは繰り返し語句など自然性の低下が見られた。ヘッドの OV/QK 分解や複数ヘッドの組み合わせ効果，多言語への一般化は今後の課題である。[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase4_patching_report.md#L68-L70)[github.com](https://github.com/toshiohanawa/emotion-circuits-project/blob/HEAD/docs/report/phase6_head_screening_report.md#L121-L124)

## 11. 今後の研究

今後は以下の点に取り組む予定である。

1. **OV/QK 回路解析** : 重要なヘッドがどのようなベクトルを書き込み，どのトークン間で注意を張るかを分析する。
2. **複数ヘッドの再構成** : 分散的な回路を抽出し，ヘッドの協調作用を明らかにする。
3. **モデル規模の拡大** : 410M〜1B パラメータのモデルで同様の実験を行い，スケール依存性を検証する。
4. **ランダムベクトル対照** : 感情方向とランダム方向を比較し，因果効果の特異性を検証する。
5. **生成の自然性評価** : 人間評価を用いて，パッチング後の文の自然さと感情表現を詳細に評価する。

## 12. 結論

本研究は，軽量 Transformer 言語モデルにおいて感情方向が明確に存在し，モデル間で共通するサブスペース構造があること，残差ストリームへのパッチングや注意ヘッドへの介入により生成文の感情トーンを因果的に操作できることを実証した。特に，Layer 1 Head 10 のようなヘッドが感情処理に大きく寄与していることが示された。これらの知見は，感情の内部回路理解に向けた基盤となるものであり，今後の機械論的解釈やモデル制御の研究に資することが期待される。
