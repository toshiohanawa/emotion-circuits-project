# Phase 8: 中規模モデル（Gemma 3 12B (google/gemma-3-12b-it)）の感情サブスペース整合性解析
_Last updated: 2025-11-16_

## 🎯 目的

gpt2 と Gemma 3 12B (google/gemma-3-12b-it) の感情サブスペース整合性を解析し、線形写像前後の overlap を比較。

## 🚀 実験設定
- source: gpt2
- target: Gemma 3 12B (google/gemma-3-12b-it)
- プロファイル: baseline
- 対象層: [0, 1, 2, 3, 4, 5, 6]（Layer 7-11は有効サンプル不足のためスキップ）
- PCA 次元 (k): 8
- 手法: token-based 感情ベクトル → 多サンプルPCA → neutral から線形写像学習 → before/after overlap
- 注意: 実行時に数値的な問題（オーバーフロー、NaN/Inf）が発生し、Layer 7-11は処理できませんでした

## 📊 結果概要（層平均）
| Layer | Mean overlap (before) | Mean overlap (after) | Δ (after - before) |
|-------|-----------------------|----------------------|--------------------|
| 0 | 0.000509 | 0.000011 | -0.000499 |
| 1 | 0.000299 | 0.000432 | 0.000133 |
| 2 | 0.000247 | 0.000261 | 0.000014 |
| 3 | 0.000235 | 0.000393 | 0.000158 |
| 4 | 0.000260 | 0.000481 | 0.000220 |
| 5 | 0.000259 | 0.000554 | 0.000294 |
| 6 | 0.000281 | 0.000325 | 0.000044 |

## 💡 考察

### 改善パターン
- **改善が非常に小さい**: 線形写像後もoverlapがほぼ0のまま（after < 0.001）
- **Layer 0で負の改善**: Layer 0では改善ではなく悪化（Δ = -0.000499）
- **平均改善幅**: 0.0001（before: 0.0003 → after: 0.0004）

### 小型モデル（Phase 6）との比較
- Phase 6やLlama3 8Bで見られた「before ≒ 0 → after > 0.1」パターンが観察されない
- Gemma3 12BとGPT-2間では、感情サブスペースの整合性が非常に低い可能性

### 技術的な問題
- **数値的な問題**: 実行時にオーバーフロー警告が発生し、Layer 7-11は処理できなかった
- **活性値のスケール**: Gemma3 12Bの活性値が非常に大きく、数値計算で問題が発生
- **アーキテクチャの違い**: Gemma3とGPT-2のアーキテクチャ差が、アライメントを困難にしている可能性

### 解釈
- Gemma3 12BとGPT-2間では、感情表現の構造が大きく異なる可能性
- または、現在の手法（neutralベースの線形写像）がGemma3には適していない可能性
- より高度なアライメント手法（CCA、非線形写像など）の検討が必要

## 🔭 今後のステップ
- サンプル数や PCA 次元を増やした再実験。
- 他モデル（Gemma3/Qwen3）との比較・横展開。
- Phase 5/7.5 の統計パイプラインに大規模モデルを統合。