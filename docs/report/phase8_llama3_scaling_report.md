# Phase 8: 中規模モデル（Llama 3.1 8B (meta-llama/Meta-Llama-3.1-8B)）の感情サブスペース整合性解析
_Last updated: 2025-11-16_

## 🎯 目的

gpt2 と Llama 3.1 8B (meta-llama/Meta-Llama-3.1-8B) の感情サブスペース整合性を解析し、線形写像前後の overlap を比較。

## 🚀 実験設定
- source: gpt2
- target: Llama 3.1 8B (meta-llama/Meta-Llama-3.1-8B)
- プロファイル: baseline
- 対象層: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
- PCA 次元 (k): 8
- 手法: token-based 感情ベクトル → 多サンプルPCA → neutral から線形写像学習 → before/after overlap

## 📊 結果概要（層平均）
| Layer | Mean overlap (before) | Mean overlap (after) | Δ (after - before) |
|-------|-----------------------|----------------------|--------------------|
| 0 | 0.000215 | 0.124622 | 0.124407 |
| 1 | 0.000180 | 0.504833 | 0.504653 |
| 2 | 0.000235 | 0.465373 | 0.465138 |
| 3 | 0.000231 | 0.432786 | 0.432555 |
| 4 | 0.000207 | 0.451560 | 0.451352 |
| 5 | 0.000222 | 0.402819 | 0.402598 |
| 6 | 0.000209 | 0.408334 | 0.408125 |
| 7 | 0.000248 | 0.418881 | 0.418632 |
| 8 | 0.000238 | 0.454718 | 0.454480 |
| 9 | 0.000193 | 0.573908 | 0.573715 |
| 10 | 0.000196 | 0.578386 | 0.578190 |
| 11 | 0.000210 | 0.713503 | 0.713292 |

## 💡 考察

### 改善パターン
- **すべての層で大幅な改善**: 線形写像によるアライメント後、すべての層でoverlapが大幅に改善（Δ = 0.12-0.71）
- **上位層ほど改善が大きい**: Layer 9-11で特に大きな改善が見られる（Layer 11: Δ = 0.713）
- **平均改善幅**: 0.443（before: 0.000220 → after: 0.443444）

### 小型モデル（Phase 6）との比較
- Phase 6（GPT-2 vs Pythia-160M）でも同様の「before ≒ 0 → after > 0.1」パターンが観察された
- Llama3 8Bでは、より大きなモデルサイズにもかかわらず、同様の改善パターンが確認された
- 改善の絶対値は、小型モデル間の比較よりも大きい（最大Δ = 0.713）

### 技術的な観察
- Layer 0でも改善が見られる（Δ = 0.124）が、上位層ほど改善が大きい
- 中間層（Layer 1-8）でも安定した改善（Δ = 0.40-0.50）
- 線形写像によるアライメントが、異なるアーキテクチャ間でも有効であることを示唆

## 🔭 今後のステップ
- サンプル数や PCA 次元を増やした再実験。
- 他モデル（Gemma3/Qwen3）との比較・横展開。
- Phase 5/7.5 の統計パイプラインに大規模モデルを統合。